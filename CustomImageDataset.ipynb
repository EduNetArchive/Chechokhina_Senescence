{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops import masks_to_boxes\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "WauIg5RESOut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvnS5hpaSNLT"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, mask_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.img_labels.iloc[idx]['Class']\n",
        "        img_path = os.path.join(self.img_dir + self.img_labels.iloc[idx]['File'])\n",
        "        mask_path = os.path.join(self.img_dir + self.img_labels.iloc[idx]['Mask'])\n",
        "\n",
        "        image =  Image.open(img_path)\n",
        "        image = np.array(image)/65535\n",
        "        image = (image - mean_train)/std_train #this is the code to normalize your single cell images\n",
        "        image_torch = torch.from_numpy(image)\n",
        "        image_torch = image_torch.unsqueeze(0)\n",
        "\n",
        "        masks = np.load(mask_path)\n",
        "        mask = np.ma.masked_values(masks, self.img_labels.iloc[idx]['Cell number']).mask\n",
        "        torch_mask = torch.unsqueeze(torch.from_numpy(mask), 0)\n",
        "        boxes = masks_to_boxes(torch_mask)\n",
        "\n",
        "        mean_x = int((boxes[0][0]+boxes[0][2])/2)\n",
        "        mean_y = int((boxes[0][1]+boxes[0][3])/2)\n",
        "\n",
        "        len_x = int((-boxes[0][0]+boxes[0][2]))\n",
        "        len_y = int((-boxes[0][1]+boxes[0][3]))\n",
        "\n",
        "        masked_cell = (image*mask)\n",
        "\n",
        "        box_cell = torch.from_numpy(masked_cell[int(boxes[0][1]):int(boxes[0][3]),\n",
        "                                        int(boxes[0][0]):int(boxes[0][2])])\n",
        "\n",
        "        box_size_x = int(-boxes[0][0])+int(boxes[0][2])\n",
        "        box_size_y = int(-boxes[0][1])+int(boxes[0][3])\n",
        "\n",
        "        x_size, y_size = 380, 380 #here you need to choose your one cell image size (depends on cell size)\n",
        "\n",
        "        if box_size_x>x_size:\n",
        "            box_cell = box_cell[:, box_size_x//2 - x_size//2: box_size_x//2+(x_size-x_size//2)]\n",
        "            box_size_y, box_size_x= box_cell.shape[0], box_cell.shape[1]\n",
        "        if box_size_y>y_size:\n",
        "            box_cell = box_cell[box_size_y//2 - y_size//2: box_size_y//2+(y_size-y_size//2), :]\n",
        "            box_size_y, box_size_x = box_cell.shape[0], box_cell.shape[1]\n",
        "\n",
        "        final_image = torch.zeros((x_size, y_size))\n",
        "        final_image[y_size//2-box_size_y//2: y_size//2+(box_size_y-box_size_y//2),\n",
        "                      x_size//2-box_size_x//2: x_size//2+(box_size_x-box_size_x//2)] = box_cell\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return final_image.unsqueeze(0), label"
      ]
    }
  ]
}
